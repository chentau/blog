<!DOCTYPE html>
<html>
	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<head>
	<meta charset="utf-8">
	<title>Exponential Families</title>
	<link rel="stylesheet" href="/blog/assets/css/main.css">
	<link rel="stylesheet" href="/blog/assets/css/syntax.css">
	<meta name="description" content="A series of notebooks on topics in machine learning and statistics.">
	<div class="header">
		<!-- <hr style="border-top:5px solid black;"> -->
		<h1 ><a href="/blog/">Completely Random Measure</a></h1>
		<hr style="border-color:mintcream;">
	</div>
</head>

	<body>
		<div class="post-contents">
	<h2 style="text-align:center;margin-top:50px;margin-bottom:5%;">Exponential Families</h2>
	<h4 style="text-align:center;margin-top:0px;margin-bottom:0px;">Tony Chen</h4>
	<h4 style="text-align:center;margin-top:0px;margin-bottom:50px;">01 Feb 2019</h4>
	<div class="post">
		<p>For my first notebook, I thought I would start with the Exponential Family, which is one of the foundations for both Bayesian and Frequentist statistics.  The range of topics spanned by the study of Exponential Families is extremely broad, which means that if you really study Exponential Families in great detail, you’ll end up learning a ton about statistics in general. In that sense, Exponential Families are worth their weight in gold, which is why I thought this would be a good starting point.  Although they might look a bit abstract at first, it turns out that they have many incredibly rich and useful properties that make them desirable for applications.</p>

<h2 id="what-are-exponential-families">What are Exponential Families?</h2>

<p>An Exponential Family is a class of distributions that can be written in the following form:</p>

<script type="math/tex; mode=display">p(x|\eta) = h(x)exp(\eta^{T}T(x) - A(\eta)</script>

<p>In the above equation, \(\eta \) is commonly referred to as the canonical parameter, T(x) is called the sufficient statistic, and \(A(\eta) \) is referred to as the log normalizer, as it is the term that ensures that the density integrates to 1.  As it turns out, a ton of very commonly used distributions (poisson, multinomial, dirichlet, beta, etc) can be written in this form.</p>

<h2 id="examples-the-bernoulli-and-gaussian-distributions">Examples: the Bernoulli and Gaussian distributions</h2>

<p>Lets start by looking at the Bernoulli distribution.  Recall that its pdf is given by:</p>

<script type="math/tex; mode=display">p(x|\pi) = \pi^{x}(1-\pi)^{1-x}</script>

<script type="math/tex; mode=display">= exp(xlog(\pi) + (1-x)log(1-\pi))</script>

<script type="math/tex; mode=display">= exp(x(log(\pi)- log(1-\pi)) + log(1-\pi))</script>

<script type="math/tex; mode=display">= exp(log(\frac{\pi}{1-\pi})x + log(1-\pi))</script>

<p>From this, we can see that the Bernoulli distribution is in the Exponential family, with natural parameter \(log(\frac{\pi}{1-\pi})\), sufficient statistic x, \(h(x)=1 \), and log normalizer \(log(1-\pi) \).</p>

<p>For another example, consider the normal distribution:</p>

<script type="math/tex; mode=display">p(x| \mu, \sigma^{2}) = \frac{1}{\sqrt{2\pi}\sigma} exp(\frac{(x-\mu)^{2}}{2\sigma^{2}})</script>

<script type="math/tex; mode=display">= \frac{1}{\sqrt{2\pi}\sigma}exp( \frac{x^{2} - 2x\mu + \mu^{2}}{2\sigma^{2}}</script>

<script type="math/tex; mode=display">= \frac{1}{\sqrt{2\pi}}exp( \frac{x^{2}}{2\sigma^{2}} - \frac{x\mu}{\sigma^{2}} + \frac{\mu^{2}}{2\sigma^{2}} - log(\sigma)</script>

<p>From this, we can see that \(h(x) = \frac{1}{\sqrt{2\pi}} \), the canonical parameters are \(\eta = [\frac{\mu}{\sigma^{2}}, -\frac{1}{2\sigma^{2}}] \), the sufficient statistics are \(T(x) = [x, x^{2}] \), and the log normalizer is</p>

<script type="math/tex; mode=display">A(\eta) = \frac{\mu^{2}}{2\sigma^{2}} - log(\sigma) = -\frac{\eta_{1}^{2}}{4\eta_{2}} - \frac{log(-2\eta_{2})}{2}</script>

<h2 id="the-mean-parameterization">The Mean Parameterization</h2>

<p>In the above two examples, we have rewritten two distributions in the exponential family representation, expressed in terms of these natural or canonical parameters \(\eta \).  However, many distributions are often parameterized by its mean, such as the normal, the bernoulli, and the poisson.  We would like to express the mean \(\mu \) in terms of the canonical parameters.  As it turns out, if we make some assumptions about linear independence and convexity, the relationship between the two is invertible, so we can recover \(\mu \) from the canonical parameters.  Take the bernoulli as an example.  We have that</p>

<p><script type="math/tex">\eta = log(\frac{\pi}{1-\pi})</script>
<script type="math/tex">\implies \mathbb{E}(X) = \pi = \frac{1}{e^{\eta} - 1}</script></p>

<p>Here we see a familiar function: it turns out that the canonical parameter of the bernoulli is linked to the mean through the logit function.  This phenomenon is what lies at the heart of Generalized Linear Models; many times, the link function of a GLM turns out to be the canonical link between the canonical parameter and the mean.  In our case, we have rediscovered the link function of logistic regression.</p>

<h2 id="sufficiency">Sufficiency</h2>

<p>At the very beginning, I mentioned that T(x) is what’s known as a sufficient statistic.  But what does that mean?  The concept of sufficiency is an incredibly rich subject, and I won’t be able to cover all of it today, although I probably will try to in a future post.  However, I will give a quick introduction.  The idea of sufficiency is grounded in the concept of data reduction.  How much can we reduce our data, without losing any key information?  Lets start with a couple of definitions.</p>

<p><strong>Def</strong>.  A <strong>statistic</strong> is any function of our data \( (x_{1}, x_{2}, \ldots) \).  The easiest example to give would be the sample mean: \( \overline{x} = \sum_{i=1}^{N} x_{i} \).</p>

<p><strong>Def</strong>. A statistic T(x) is called <strong>sufficient</strong> if the distribution of our data does not depend on its parameter \( \theta \); ie \( p(X \vert T, \theta) = p(X \vert T) \).</p>

<p>Intuitively, we can think of this as implying that \(T(x) \) contains all of the information we need to know about the distribution of our data.  I claim that the T(x) in the exponential family representation is a sufficient statistic. The answer to that claim lies in a theorem given by Neyman.</p>

<p><strong>Theorem</strong>.  A statistic T(x) is sufficient if the distribution of the data can be factorized as follows:</p>

<script type="math/tex; mode=display">p(X \vert \theta) = h(x) g(\theta, T(x))</script>

<p>Pf.  Some other time.</p>

<p>When we look at how the exponential family is written, we can see that</p>

<script type="math/tex; mode=display">p(x|\eta) = h(x)exp(\eta^{T}T(x) - A(\eta)</script>

<script type="math/tex; mode=display">= h(x)g(\eta, T(x))</script>

<p>And so we have that \(T(x) \) is a sufficient statistic.  With respect to exponential families, \(T(x) \) has a variety of uses and interpretations, however the primary one that I will focus one, is the part that it plays in inference.</p>

<h2 id="moments">Moments</h2>

<p>An interesting property of \(A(\eta) \) is that it generates the moments of the sufficient statistic \(T(x) \). In that sense, \(A(\eta) \) acts like a moment generating function of sorts.  We have that the normalizer is written as</p>

<script type="math/tex; mode=display">A(\eta) = log \int h(x)exp(\eta^{T}T(x))dx</script>

<p>Take the derivative with respect to \(\eta \):</p>

<script type="math/tex; mode=display">\frac{\partial }{\partial \eta}A(\eta) = \frac{1}{\int h(x)exp(\eta^{T}T(x))} * \int h(x)T(x)exp(\eta^{T}T(x))</script>

<p>Here, we’ve actually made some assumptions, by interchanging the derivative and integral.  We can’t do this in general, but it turns out to be ok in this scenario.</p>

<script type="math/tex; mode=display">= \frac{\int h(x)T(x)exp(\eta^{T}T(x))}{exp(A(\eta))}</script>

<script type="math/tex; mode=display">= \int T(x)h(x)exp(\eta^{T}T(x) - A(\eta)) = \mathbb{E}(T(x))</script>

<p>If we were to take the second partials, we would get</p>

<script type="math/tex; mode=display">\frac{\partial^{2}}{\partial \eta^{2}}A(\eta) = \int T(x)h(x)[T(x) - \frac{\partial}{\partial \eta}A(\eta)]^{T}exp(\eta^{T}T(x) - A(\eta))</script>

<p>Where again we have interchanged differentiation and integration.</p>

<script type="math/tex; mode=display">= \int T(x)h(x)[T(x) - \mathbb{E}(T(x))]exp(\eta^{T}T(x) - A(\eta))</script>

<script type="math/tex; mode=display">= \mathbb{E}(T(x)T(x)^{T} - T(x)\mathbb{E}(T(x))^{T} )</script>

<script type="math/tex; mode=display">= \mathbb{E}(T(x)T(x)^{T}) - \mathbb{E}(T(x))\mathbb{E}(T(x))^{T} = Var(T(x))</script>

<h2 id="inference">Inference</h2>

<p>Arguably, the most important property of exponential families are how nicely they play with inference.  Lets start by looking at a frequentist MLE for the natural parameters.  First, lets look at the distribution of the data.</p>

<script type="math/tex; mode=display">p(\mathbf{x} |\eta) = \prod_{i=1}^{N} h(x_{i}) = \prod_{i=1}^{N}h(x_{i}) exp(\eta^{T}(\sum_{i=1}^{N}T(x_{i})) - NA(\eta))</script>

<p>Thus, we can see that the joint distribution of our data, is also an exponential family distribution with updated sufficient statistics.  Lets go ahead and take the log of this, to derive our log likelihood</p>

<script type="math/tex; mode=display">l(\eta) = \sum_{i=1}^{N} h(x_{i}) + \eta^{T}\sum_{i=1}^{N}T(x_{i}) - NA(\eta)</script>

<p>Then, take the derivative with respect to eta and set equal to 0:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial \eta}l(\eta) = 0 = \sum_{i=1}^{N}T(x_{i}) - N\frac{\partial}{\partial \eta}A(\eta)</script>

<script type="math/tex; mode=display">\implies \frac{\partial }{\partial \eta}A(\eta) = \mathbb{E}(T(x)) = \frac{1}{N}\sum_{i=1}^{N}T(x_{i})</script>

<p>Thus, we have that the maximum likelihood estimate for \(\eta \) is simply the one that equates the expectation of the sufficient statistic with the sample mean of the sufficient statistic.  For distributions such as the bernoulli or poisson, where the sufficient statistic is simply x itself, this implies that the MLE of the parameter is simply the sample mean.  I won’t derive it here, but its decently simple to show that this MLE is both unbiased and efficient - that is, it achieves the Cramer Rao Lower Bound.</p>

<p>Finally, we have the property of conjugacy.  The idea is that essentially, if you have that your data is distributed according to an exponential family with parameter \(\eta \), then you can always (in theory) find another exponential family prior for \(\eta \) such that the posterior of \(\eta \) is another exponential family.  The argument goes like this:  let your data have some exponential family representation with natural parameters \(\eta \).</p>

<script type="math/tex; mode=display">p(\mathbf{x} | \eta) = \prod_{i=1}^{N} h(x_{i})exp(\eta^{T}\sum_{i=1}^{N}T(x_{i}) - NA(\eta))</script>

<p>Then, define a prior for \(\eta \) with two hyperparameters \(\alpha, \beta \) to be</p>

<script type="math/tex; mode=display">p(\eta | \alpha, \beta) = h(\eta)exp(\alpha\eta - \beta(A(\eta)) - A(\alpha, \beta))</script>

<p>We have that \(\alpha, \beta \) form the natural parameters, and \(\eta, -A(\eta) \) are our sufficient statistics, and the whole thing is an exponential family.  Lets go ahead and compute the posterior.</p>

<script type="math/tex; mode=display">p(\eta| \mathbf{x}, \alpha, \beta) \propto p(\mathbf{x} | \eta) * p(\eta | \alpha, \beta) \propto exp(\eta(\alpha + \sum_{i=1}^{N}T(x_{i})) - A(\eta)(\beta + N) - A(\alpha, \beta))</script>

<p>Note that this is also an exponential family, with updated parameters \(\hat{\alpha} = \alpha + \sum_{i=1}^{N}T(x_{i}), \hat{\beta} = \beta + N \)</p>

<p>This is an extremely nice property, since it allows us to define priors such that we have an analytic form for the posterior.  Thus, we don’t have to resort to approximate inference of sampling methods, and derive the posterior manually.  However, note that there are some limitations.  Deriving the exponential family prior required that we specify two hyperparameters (or more generally, dim(\(\eta \)) + 1 hyperparameters) and as such, we can’t just go around and tossing random exponential family distributions together and expect a closed form posterior.  However, many applications of bayesian statistics can be formulated in terms of conjugate priors, thus saving a lot of time and energy.  Some examples include Bayesian linear regression, Beta Bernoulli model, and more.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Exponential families are an incredibly versatile family of distributions that have some extremely nice theoretical properties, foremost among them the property of conjugacy, which allows for Bayesian models to be formulated such that the posterior has an analytic solution.  However, there are so many other aspects to them that I haven’t addressed in the notebook.  Exponential families are definitely worth their weight in gold in terms of output per hour spent, probably due to their weird tendency to unite various branches of statistics in their study.  For example, the study of Exponential Families in full gives rise to Hypothesis Testing, Maximum Entropy, Unbiased and Efficient estimators, and so much more. As such, I would highly recommend anyone reading this to familiarize themselves with Exponential Families in full.  This notebook was derived mostly from David Blei’s notes at Princeton, and Michael Jordan’s textbook <em>An Introduction to Probabilistic Graphical Models</em>.</p>

	</div>
</div>

	<footer>
	<div class="footer">
		<hr style="border-color:mintcream;">
		<a style="display:block;" href="https://http://localhost:4000"/>about</a>
	</div>
</footer>

	</body>
</html>
