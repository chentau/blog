<!DOCTYPE html>
<html>
	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<head>
	<meta charset="utf-8">
	<title>Variational Inference Part 1&#58; Introduction and Coordinate Ascent</title>
	<link rel="stylesheet" href="/blog/assets/css/main.css">
	<link rel="stylesheet" href="/blog/assets/css/syntax.css">
	<div class="header">
		<hr style="border-top:5px solid black;">
		<h1 ><a href="/blog/">Completely Random Measure</a></h1>
		<hr>
	</div>
</head>

	<body>
		<div class="post-contents">
	<h2 style="text-align:center;margin-top:50px;margin-bottom:5%;">Variational Inference Part 1&#58; Introduction and Coordinate Ascent</h2>
	<h4 style="text-align:center;margin-top:0px;margin-bottom:0px;">Tony Chen</h4>
	<h4 style="text-align:center;margin-top:0px;margin-bottom:50px;">04 Feb 2019</h4>
	<div class="post">
		<p>The biggest challenge of Bayesian Statistics lies in inference, where one combines the prior and likelihood to get a posterior.  Most of the time, the posterior will not have an analytic form and so we have to rely on sampling methods such as MCMC.  However, MCMC can be very computationally expensive, and in the age of big data, there is a lot of attention being directed towards developing new inference algorithms that could scale well to large datasets.  In the last couple of years, Variational Inference has become a viable and scaleable alternative to MCMC for performing inference on complex Bayesian models.  In this notebook, I’ll present the basic math underlying the mechanics of Variational Inference, and also present it in its most simple form: Coordinate Ascent Variational Inference.  I’ll end with a practical example, showing how to apply Coordinate Ascent Variational Inference to a Mixture of Gaussians model.  My derivations will be following <a href="https://arxiv.org/abs/1601.00670">(Blei 2016)</a> very closely.</p>

<h2 id="introduction-and-the-problem">Introduction and the Problem</h2>

<p>Recall Bayes rule, which gives us the procedure for drawing inferences from data:</p>

<script type="math/tex; mode=display">p(\theta \vert D) = \frac{p(D \vert \theta)p(\theta)}{p(D)}</script>

<p>The term in the denominator is often referred to as the marginal likelihood, or the evidence, and is what causes inference to be intractable.  This is because to calculate \(p(D) \), we have to integrate over all of our parameters:  \(p(D) = \int_{\theta \in \Theta} p(D \vert \theta)p(\theta) \).  Now if \(\theta \) is one dimensional this could work, but as the dimensionality of our parameter space increases, the computational power needed also increases exponentially, making this an unfeasible numerical computation.</p>

<p>Variational Inference, or VI, belongs to a class of inference algorithms referred to as approximate inference, which is designed to get around this problem in a different way from MCMC.  The idea is that instead of drawing samples from the exact posterior, such as MCMC, you trade off a little bit of bias in exchange for computational tractability.  As such, these approximate inference methods transform inference from a sampling problem, to an optimization problem, which is typically much more tractable and easier to deal with.  VI is just one of the approximate sampling algorithms, but it has been the one that has seen the most success and has been the most widely adopted.</p>

<h2 id="statement-of-the-objective">Statement of the objective</h2>

<p>Like previously stated, VI transforms inference from a sampling problem into a optimization problem.  It does this by approximating the posterior \(p(\theta \vert x) \) with some class of distributions Q parameterized by what we call “variational parameters” \(\xi \).  Note that the dimension of \(\xi \) depends on what variational distribution we choose. Then, we want to find \(\xi \) such that our variational distribution \(q(\theta \vert \xi)\) is “close” to the posterior.  I’ll go ahead and note here that technically, our variational distribution is a function of x; the exact notation should probably be \(q(\theta \vert \xi(x) \)), but you’ll often see \(q(\theta) \text{ or } q(\theta \vert \xi) \) for brevity. Going back to the problem of defining “closeness”, the canonical measure that is used is the KL Divergence.  Recall that for two distributions p, q, the KL divergence is defined as:</p>

<script type="math/tex; mode=display">KL(p || q) = \int p(x) log\, \frac{p(x)}{q(x)}dx</script>

<p>Our objective then becomes</p>

<script type="math/tex; mode=display">\text{Find } q^{*}(\theta) = \underset{q \in Q}{argmin} KL(q(\theta) || p(\theta | x))</script>

<p>And then once we have found \(q^{*}(\theta) \), we we use that as our approximate posterior.  Again, I’ll stress that theres an implicit \(\xi \) in the distribution q; so when I say to find the distribution that maximizes this quantity, I really mean find \(\xi \) that maximizes \(KL(q(\theta \vert \xi) \vert\vert p(\theta \vert x)) \).</p>

<p>Now that we have our objective, everything should be fine and dandy.  However, we run into a big problem when we actually try and solve this.  Lets go ahead expand out the definition of the KL divergence:</p>

<script type="math/tex; mode=display">KL(q(\theta) || p(\theta | x)) = \int q(\theta) log\, \frac{q(\theta)}{p(\theta)}</script>

<script type="math/tex; mode=display">= \mathbb{E}_{q}[log\, \frac{q(\theta)}{p(\theta)}]</script>

<script type="math/tex; mode=display">= \mathbb{E}_{q}[log\, q(\theta)] - \mathbb{E}_{q}[log\, p(\theta| x)]</script>

<script type="math/tex; mode=display">= \mathbb{E}_{q}[log\, q(\theta)] - \mathbb{E}_{q}[log\, p(\theta, x)] - \mathbb{E}_{q}[log\, p(x)]</script>

<p>We can see that the undesirable term, \(p(x) \) pops up again in this equation.  Thus we have that the KL divergence is intractable and useless to us.</p>

<p>What can we do from here?  The way forward is to note that since \(p(x) \) is a constant with respect to theta, we can drop it from the whole term and minimize the KL divergence up to an additive constant.</p>

<h2 id="the-elbo">The ELBO</h2>

<p>Define the Evidence Lower Bound (ELBO) as</p>

<script type="math/tex; mode=display">ELBO(q) = \mathbb{E}[log\, p(\theta,x)] - \mathbb{E}[log\, q(\theta)]</script>

<p>Where the expectations are taken with respect to q.  It is imperative, that you keep track of what expectations are being taken with respect to which distributions and what variables.  If you don’t have a solid grasp of this, then all of these next few derivations are going to be extremely hard.  It might seem a bit weird to be taking the expectation of a distribution, but here its just easier to think of \( p(\mathbf{\theta}, x)\) as some function of \(\theta \) with x held fixed.  We can see that the EBLO is simply the negative KL divergence plus some additive constant.  Thus, maximizing the ELBO will minimize the KL divergence, which will then give us the variational distribution that we want.  Furthermore, because we have dropped the intractable constant, we can actually compute the ELBO.</p>

<p>In addition to being our optimizational objective, the ELBO also has the nice property of providing a lower bound for the marginal likelihood/evidence (hence the name).  To see this, observe that</p>

<script type="math/tex; mode=display">log(p(x)) = log \int p(x\vert \theta)p(\theta)dx = log \int p(x, \theta)dx</script>

<script type="math/tex; mode=display">= log \int p(x, \theta) \frac{q(\theta)}{q(\theta)}</script>

<script type="math/tex; mode=display">= log \int q(\theta) \frac{p(x, \theta)}{q(\theta)}</script>

<script type="math/tex; mode=display">= log\, \mathbb{E}[\frac{p(x, \theta)}{q(\theta)}] \leq \mathbb{E}[log p(x, \theta)] - \mathbb{E}[log\, q(\theta))]</script>

<p>Here, the above inequality followed from application of Jensen’s inequality.</p>

<p>Thus, we have formulated a tractable variational objective: find the distribution \(q(\theta \vert \xi) \in Q \) that maximizes the ELBO, which will in turn minimize the KL divergence between \(q \) and the posterior.</p>

<h2 id="the-mean-field-assumption-and-the-coordinate-ascent-updates">The Mean Field Assumption and the Coordinate Ascent Updates</h2>

<p>Now that we have the function we want to maximize, the next step is to figure out how to actually maximize this thing.  The easiest, and arguably simplest way, is to apply coordinate ascent, where we maximize with respect to one variable at each sweep, holding all others constant.  Lets start by clarifying my notation.  Let \(\theta \) be the vector of all parameters, and \(\theta_{j} \) be the jth parameter in the parameter vector.  Similarly, let \(\mathbf{x} \) be the vector of observed data and \(x_{i} \) be the ith datapoint.  Let \(-j \) denote everything except for j.  For example, <script type="math/tex">\mathbb{E}_{-j} [\mathbf{ \theta_{-j}}]</script> would represent the expectation of all of the theta parameters, except for the jth one.  With all that out of the way, lets get started.</p>

<p>First, we make a simplifying assumption.  In what we call the <strong>mean field assumption</strong>, we assume that the joint variational distributions of our parameters decomposes into the product of the marginals:</p>

<script type="math/tex; mode=display">q(\mathbf{\theta}) = \prod_{i=1}^{k}q(\theta_{i})</script>

<p>As we will see, this simplifies the derivations greatly.  However, at the same time, it also completely ignores the covariance between the parameters, which is why you will often hear people make the statement that VI significantly underestimates the variance of its estimated parameters.  This is very true, and is something to always keep in mind when you apply VI to some modeling problem.</p>

<p>Lets go ahead and manually maximize the ELBO.  I claim that the optimal update for the variational distribution of \(q(\theta_{i}) \) takes the form</p>

<script type="math/tex; mode=display">q^{*}(\theta_{j}) \propto exp(\mathbb{E}_{-j}[log\, p(\theta_{j} \vert \theta_{-j}, \mathbf{x})])</script>

<p>Where the expectation is taken with respect to the variational distributions of the parameters (not the actual distributions)!  Note that we can rewrite the term in the expectation in a bunch of different ways: this is because <script type="math/tex">p(\theta_{j} \vert \theta_{-j}, \mathbf{x}) \propto p(\theta_{j}, \theta_{-j} \vert \mathbf{x}) \propto p(\theta_{j}, \theta_{-j}, \mathbf{x})</script> and so on.  And, because it is a proportionality symbol, this is not a true distribution, but rather something that is <em>proportional</em> to a probability distribution.  Begin by rewriting the ELBO as a function of \(\theta_{j} \):</p>

<script type="math/tex; mode=display">ELBO(q) = \mathbb{E}[log\, p(\theta,\mathbf{x})] - \mathbb{E}[log\, q(\theta)] =\mathbb{E}[log\, p(\theta,\mathbf{x})] - \mathbb{E}[\sum_{i} log\, q(\theta_{i})]</script>

<script type="math/tex; mode=display">= \mathbb{E}[log\, p(\mathbf{\theta_{-j}}, \theta_{j}, \mathbf{x})] - \mathbb{E}_{j}[log\, q(\theta_{j})] + const</script>

<p>First I’ll clarify that the first expectation is a k dimensional integral, since we have k parameters.  I’m going to go ahead and apply iterated expectation to our first term, which then turns into</p>

<script type="math/tex; mode=display">= \mathbb{E}[\mathbb{E}_{-j}[log\, p(\theta_{-j}, \theta_{j}, \mathbf{x}) \vert \theta_{-j}]] - \mathbb{E}_{-j}[log\, q(\theta_{j})] + const</script>

<p>Expand the inner term <script type="math/tex">\mathbb{E}_{-j}[log\, p(\theta_{-j}, \theta_{j}, \mathbf{x}) \vert \theta_{-j}] = \int q(z_{-j} \vert z_{j})(log\, p(\theta_{j}, \theta_{-j}, \mathbf{x})) = \int q(z_{-j})log\, p(\theta_{j}, \theta_{-j}, \mathbf{x})</script> where the last step followed because of our mean field assumption.  Thus, the ELBO becomes</p>

<script type="math/tex; mode=display">= \mathbb{E}[ \mathbb{E}_{-j}[ log \, p(\theta_{j}, \theta_{-j}, \mathbf{x})]] - \mathbb{E}_{j}[log\, q(\theta_{j})] + const</script>

<script type="math/tex; mode=display">= \mathbb{E}_{j}[\mathbb{E}_{-j}[log \, p(\theta_{j}, \theta_{-j}, \mathbf{x})]] - \mathbb{E}_{j}[log \, q(\theta_{j})] + const</script>

<script type="math/tex; mode=display">= \mathbb{E}_{j}[log\, \exp(\mathbb{E}_{-j}[log\, p(\theta_{j}, \theta_{-j}, \mathbf{x}))]] -\mathbb{E}_{j}[log\, q(\theta_{j})] + const</script>

<script type="math/tex; mode=display">= -\mathbb{E}_{j}[log\, \frac{q(\theta_{j})}{\exp(\mathbb{E}_{-j}[log \, p(\theta_{j}, \theta_{-j}, \mathbf{x})])}]</script>

<p>We can see that this takes the form of the negative KL Divergence between <script type="math/tex">q(\theta_{j})</script> and <script type="math/tex">\exp(\mathbb{E}_{-j}[log \, p(\theta_{j}, \theta_{-j}, \mathbf{x})]) = q^{*}(\theta)</script> plus some additive constant.   Thus, we maximize the ELBO when we minimize the above equation: ie. when we set \(q(\theta_{j}) = q^{*}(\theta_{j}) \).</p>

<p>At this point, lets stop, take a deep breath, and recap our algorithm.  At each iteration, set each variational parameter proportional to <script type="math/tex">\exp(\mathbb{E}_{-j}[log\, p(\theta_{j}, \theta_{-j}, \mathbf{x}))</script>.  Then, calculate the ELBO <script type="math/tex">\mathbb{E}[log\, p(\theta,x)] - \mathbb{E}[log\, q(\theta)]</script>.  Repeat until convergence.  Now that we’ve gone through the mechanics of how Coordinate Ascent Variational Inference works, lets go ahead and see a few examples.</p>

<h2 id="example-1-mixture-of-gaussians">Example 1: Mixture of Gaussians</h2>

<p>Consider a classic mixture model with K components, that has this setup:</p>

<script type="math/tex; mode=display">\mu \sim \text{N}(0,\sigma^{2})</script>

<script type="math/tex; mode=display">c \sim \text{Multinomial}(1, [\frac{1}{K} , \ldots \frac{1}{K}])</script>

<script type="math/tex; mode=display">x|\theta,c \sim \text{N}(c^{T}\theta, 1)</script>

<p>The first step is to determine which variational distributions we place on each of the parameters.  I’m going to go with a normal variational distribution for the component means, and a categorical distribution for the cluster assignments.  Note that the variational distributions do not have to be the same as the prior distributions; I could have picked any distribution I wanted to be the variational distribution.  In practice however, its a good idea to have the support of your variational distribution and parameter space line up.  My variational parameters are going to be the mean and variance of the normal variational distribution, and the probability vector for the categorical distribution.  Therefore, our variational inference model is specified by <script type="math/tex">q(\mu \vert m, s^{2}) = \text{N}(m, s^2)</script> and <script type="math/tex">q(c \vert \phi) = \text{Multinomial}(1, \phi)</script>.</p>

<p>Lets start by deriving the update equations for <script type="math/tex">c</script> first.  We have that for the ith cluster assignment (corresponding to the ith person), the joint distribution can be factorized as follows:</p>

<script type="math/tex; mode=display">p(\mathbf{x}, c_{i}, c_{-i}, \mu) = p(c_{i})p(\mu \vert c_{i})p(c_{-i} \vert \mu, c_{i})p(\mathbf{x} \vert \mu, c_{-i}, c_{i}) = p(c_{i})p(\mu)p(c_{i})p(x_{i} \vert \mu, c_{i}) \propto p(c_{i})p(\mathbf{x} \vert \mu, c_{i})</script>

<p>Where in the last step we have removed all of the terms that do not depend on i.  Therefore, the update rule becomes</p>

<script type="math/tex; mode=display">q^{*}(\phi_{i}) \propto \exp(\mathbb{E}[log\, p(c_{i})] + \mathbb{E}[log\, p(\mathbf{x} \vert \mu, c_{i})]) = \exp(-log\, K + \mathbb{E}[\log\, p(\mathbf{x} \vert \mu, c_{i})]) \propto \exp(\mathbb{E}[log\, p(x_{i} \vert \mu, c_{i})])</script>

<p>I didn’t write it down because I’m too lazy, but recall that the expectation is taken with respect to everything except for the ith component - in this case, all of the mixture means.  Obviously its taken with respect to the other cluster assignments too, but this is a constant with respect to that and so those terms in the expectation fall away.</p>

<p>Lets focus in on that expectation term.  The key fact here, is noting that <script type="math/tex">p(x_{k} \vert \mu, c_{i}) = \prod_{i=1}^{K}p(x_{i} \vert \mu_{i})^{c_{ik}}</script>.  Thus, this becomes</p>

<script type="math/tex; mode=display">\mathbb{E}[log\, p(x_{i} \vert c_{i}, \mu)] = \sum_{k=1}^{K}c_{ik} \mathbb{E}[log\, p(x_{i} | \mu_{k}, c_{ik})]</script>

<script type="math/tex; mode=display">\propto \sum_{k=1}^{K}c_{ik}\mathbb{E}[-\frac{(x_{i} - \mu_{k})^{2}}{2}]</script>

<script type="math/tex; mode=display">\propto \sum_{k=1}^{K}c_{ik}\mathbb{E}[-\frac{x_{i}^{2} - 2x_{i}\mu_{k} + \mu_{k}^{2}}{2}]</script>

<script type="math/tex; mode=display">\propto \sum_{k=1}^{K}c_{ik}( \mathbb{E}[\mu_{k}]x_{i} - \mathbb{E}[\mu_{k}^{2}]/2)</script>

<p>I’m going to stress again that the expectations here are with respect to the variational distribution of <script type="math/tex">\mu_{k}</script>; that is, a normal with mean m and variance <script type="math/tex">s^{2}</script>.  Thus, <script type="math/tex">\mathbb{E}[\mu_{k}] = m, \mathbb{E}[\mu_{k}^{2}] = s^{2} + m^{2}</script>.  When we put this expectation back into the update equation, we find that</p>

<script type="math/tex; mode=display">q^{*}(c_{ik}) \propto \exp(c_{ik}[m_{k}x_{i} - (m_{i}^2 + s_{i}^2)/2]) = \exp(c_{ik}\, log\, \exp( [m_{k}x_{i} - (m_{i}^2 + s_{i}^2)/2]))</script>

<p>If we stare at this long enough, we can notice that this is actually the exponential family representation for the multinomial, with natural parameter \(\eta_{k} = log, p_{k}= \exp( [m_{k}x_{i} - (m_{i}^2 + s_{i}^2)/2])) \).  This implies that the optimal parameter value is given by \( \phi_{ik} \propto \exp(m_{k}x_{i} - (m_{i}^2 + s_{i}^2)/2)\).  Note that we still have to normalize this, to enforce the constraints that\(\sum_{k} \phi_{ik} = 1 \).</p>

<p>Then, we’ll derive the updates for the means.</p>

<script type="math/tex; mode=display">p(\mathbf{x}, \mu_{k}, \mu_{-k}, \mathbf{c}) = p(\mu_{k})p(\mu_{-k} \vert \mu_{k})p(\mathbf{c} \vert \mu)p(\mathbf{x} \vert \mu, \mathbf{c}) \propto p(\mu_{k})p(\mathbf{x} \vert \mu, \mathbf{c}) = p(\mu_{k})\prod_{i=1}^{N}p(x_{i} \vert \mu, c_{i})</script>

<p>Our update is</p>

<script type="math/tex; mode=display">q^{*}(\mu_{k}) \propto \exp(log \, p(\mu_{k}) + \mathbb{E}[\sum_{i=1}^{N}log \, p(x_{i} \vert \mu, c_{i}))]</script>

<script type="math/tex; mode=display">\propto \exp(\frac{-\mu_{k}^2}{2\sigma^{2}} + \sum_{i=1}^{N}\mathbb{E}[ log\, c_{ik}p(x_{i} \vert \mu_{k})]</script>

<script type="math/tex; mode=display">\propto \exp(\frac{-\mu_{k}^2}{2\sigma^{2}} + \sum_{i=1}^{N}\phi_{ik}(-\frac{(x_{i}- \mu_{k})^{2}}{2})</script>

<script type="math/tex; mode=display">\propto \exp(\frac{-\mu_{k}^2}{2\sigma^{2}} + \sum_{i=1}^{N}\phi_{ik}x_{i}\mu_{k} - \phi_{ik}\mu_{k}^{2}/2</script>

<script type="math/tex; mode=display">= \exp([\sum_{i}\phi_{ik}x_{i}]\mu_{k} - [\frac{1}{2\sigma^2} + \sum_{i}\frac{\phi_{ik}}{2}]\mu_{k}^2)</script>

<p>Again, lets stare at this thing.  We can again see that this is precisely the exponential family representation of a gaussian!  This brings up an interesting question: does this result hold in general?  Very interesting question indeed …</p>

<p>We can see from the above equation that the natural parameters are <script type="math/tex">\eta_{1} = \sum_{i}\phi_{ik}x_{i}, \; \eta_{2} = -\frac{1}{2\sigma^2} - \sum_{i}\frac{\phi_{ik}}{2}</script>.  Furthermore, recall that the mean parameterization of a normal is given by <script type="math/tex">\mu = -\frac{\eta_{1}}{\eta_{2}}, \; \sigma^{2} = -\frac{1}{\eta_{2}}</script>.  From this, we derive the parameters of our optimal variational distribution as</p>

<script type="math/tex; mode=display">m_{k} = \frac{\sum_{i}\phi_{ik}x_{i}}{\frac{1}{\sigma^2} + \sum_{i}\phi_{ik}}, s_{k}^2 = \frac{1}{\frac{1}{\sigma^2} + \sum_{i}\phi_{ik}}</script>

<p>Now, the final step is to derive the ELBO.  Generally, its a good idea to derive the ELBO first, but I decided to put it at the end, because once we have all of the component pieces, its only a matter of combining them to reproduce the ELBO.</p>

<p>We have that <script type="math/tex">ELBO = \mathbb{E}[log\, p(\mathbf{x},\mu,c)] - \mathbb{E}[log\, q(\mu, c)]</script>.  Lets expand out the first term:</p>

<script type="math/tex; mode=display">\mathbb{E}[log\, p(\mathbf{x},\mu,c)] = \mathbb{E}[log\, p(\mu)] + \mathbb{E}[log\, p(c)] + \mathbb{E}[log\, p(\mathbf{x} \vert \mu, c)]</script>

<script type="math/tex; mode=display">= \sum_{i}\mathbb{E}[log\, p(c_{i})] + \mathbb{E}[log\, p(x_{i} \vert \mu, c_{i})] + \sum_{k}\mathbb{E}[log\, p(\mu_{k})]</script>

<script type="math/tex; mode=display">\propto \sum_{i}\sum_{k}\mathbb{E}[c_{ik}[-(\frac{x_{i} - \mu_{k})^2}{2}] + \sum_{k}\mathbb{E}[-\frac{\mu_{k}^2}{2\sigma^2}]</script>

<script type="math/tex; mode=display">\propto \sum_{i}\sum_{k}\phi_{ik}[-\frac{x_{i}^2}{2} + m_{k}x_{i} - \frac{m_{k}^{2} + s_{k}^2}{2}] - \sum_{k}\frac{m_{k}^2 + s_{k}^2}{2\sigma^2}</script>

<p>As for the second term, we have:</p>

<script type="math/tex; mode=display">\mathbb{E}[log\, q(\mu, c)] \propto \sum_{k}\mathbb{E}[-\frac{(\mu_{k} - m_{k})^2}{2s_{k}^2} - \frac{1}{2}log\, s_{k}^{2}] + \sum_{i}\phi_{ik}log\, \phi_{ik}</script>

<script type="math/tex; mode=display">= \sum_{k}-\mathbb{E}[\frac{\mu_{k}^{2} - 2\mu_{k}m_{k} + m_{k}^{2}}{2s_{k}^{2}} - \frac{1}{2}log\, s_{k}^{2}] + \sum_{i}\phi_{ik}log\, \phi_{ik}</script>

<script type="math/tex; mode=display">= \sum_{k}[\frac{-m_{k}^{2} + s_{k}^{2} + 2m_{k}^{2} - m_{k}^{2}}{2s_{k}^{2}} + \frac{1}{2}log\, s_{k}^{2}] + \sum_{i}\phi_{ik}log\, \phi_{ik}</script>

<script type="math/tex; mode=display">= \propto - \frac{log\, s_{k}^{2}}{2} + \sum_{i}\phi_{ik}log\, \phi_{ik}</script>

<p>Putting it together, we have:</p>

<script type="math/tex; mode=display">ELBO = \sum_{i}\sum_{k}\phi_{ik}[-\frac{x_{i}^2}{2} + m_{k}x_{i} - \frac{m_{k}^2}{2}] + \frac{log\, s_{k}^2}{2}   - \sum_{k}\frac{m_{k}^2 + s_{k}^2}{2\sigma^2} + \phi_{ik}log\, \phi_{ik}</script>

<p>And finally, we are done with our derivation.  For me at least, this topic was something that I had to stew on for a while before I truly began to understand it, so it’s totally fine if it all seems a bit confusing at first.</p>

<p>Now, lets go ahead and see an implementation of our algorithm.  First, we’ll generate some data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	<span class="kn">import</span> <span class="nn">torch</span>
	<span class="kn">import</span> <span class="nn">torch.distributions</span>
	<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">plt</span>
	<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

	<span class="n">datapoints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
	<span class="n">datapoints</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">333</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">333</span><span class="p">,))</span>
	<span class="n">datapoints</span><span class="p">[</span><span class="mi">333</span><span class="p">:</span><span class="mi">666</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">333</span><span class="p">,))</span>
	<span class="n">datapoints</span><span class="p">[</span><span class="mi">666</span><span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">333</span><span class="p">,))</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">333</span><span class="p">]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">333</span><span class="p">:</span><span class="mi">666</span><span class="p">]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">666</span><span class="p">:]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/images/gmm_hist.png" alt="histogram" /></p>

<p>And here is the code to fit the mixture model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	
	<span class="kn">import</span> <span class="nn">torch</span>
	<span class="kn">import</span> <span class="nn">torch.distributions</span>

	<span class="s">"""
	phi: K x N matrix of cluster assignments
	mu: K x 1 vector of cluster means
	x: N x 1 vector of data points
	"""</span>

	<span class="k">def</span> <span class="nf">update_phi</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
		<span class="s">"""
		Variational update for the mixture assignments c_i
		"""</span>
		<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
		<span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">s2</span><span class="o">+</span><span class="n">m</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mf">.5</span>
		<span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">update_m</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>
		<span class="s">"""
		Variational update for the mean of the mixture mean
		distribution mu
		"""</span>
		<span class="n">num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
		<span class="n">denom</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">sigma_sq</span> <span class="o">+</span> <span class="n">phi</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">num</span><span class="o">/</span><span class="n">denom</span>

	<span class="k">def</span> <span class="nf">update_s2</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>
		<span class="s">"""
		Variational update for the variance of the mixture mean
		distribution mu
		"""</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">sigma_sq</span> <span class="o">+</span> <span class="n">phi</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">compute_elbo</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>
		<span class="c1"># The ELBO
</span>		<span class="n">t1</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_sq</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">m</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">s2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
		<span class="n">t2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">phi</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> \
				<span class="o">-</span><span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">m</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">s2</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">phi</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phi</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
		<span class="k">return</span> <span class="n">t1</span> <span class="o">+</span> <span class="n">t2</span>

	<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">num_iter</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">):</span>
		<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
		<span class="c1"># Randomly initialize the parameters
</span>		<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">k</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
		<span class="n">s2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">k</span><span class="p">)])</span>
		<span class="n">phi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
		<span class="n">elbo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iter</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
			<span class="n">phi</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span><span class="n">k</span><span class="p">)))</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">):</span>
			<span class="n">phi</span> <span class="o">=</span> <span class="n">update_phi</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
			<span class="n">m</span> <span class="o">=</span> <span class="n">update_m</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">)</span>
			<span class="n">s2</span> <span class="o">=</span> <span class="n">update_s2</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">)</span>
			<span class="n">elbo</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_elbo</span><span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">)</span>
		<span class="k">return</span> <span class="p">(</span><span class="n">phi</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">elbo</span><span class="p">)</span>
		
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	<span class="kn">import</span> <span class="nn">gmm</span>
	<span class="n">out</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">num_iter</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">333</span><span class="p">]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">333</span><span class="p">:</span><span class="mi">666</span><span class="p">]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">datapoints</span><span class="p">[</span><span class="mi">666</span><span class="p">:]),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
	<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))),</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/blog/assets/images/gmm_fit.png" alt="Fitted model" /></p>


	</div>
</div>

	<footer>
	<div class="footer">
		<hr style="color:gray">
		<a style="display:block;" href="https://http://localhost:4000"/>about</a>
	</div>
</footer>

	</body>
</html>
